<!-- TOC -->

- [Introduction](#introduction)
    - [Abbreviations used in this document](#abbreviations-used-in-this-document)
    - [What is KPS?](#what-is-kps)
    - [Problems with plain prometheus](#problems-with-plain-prometheus)
- [FAQ](#faq)
    - [I want to setup an alert to my channel](#i-want-to-setup-an-alert-to-my-channel)
    - [I want to monitor ...](#i-want-to-monitor-)
    - [How we generate KPS resources ?](#how-we-generate-kps-resources-)
    - [Note for infra team](#note-for-infra-team)
        - [Example: deploying alertmanager instance](#example-deploying-alertmanager-instance)
        - [Example: deploying prometheus instance](#example-deploying-prometheus-instance)
        - [Upgrading prometheus-operator](#upgrading-prometheus-operator)
        - [PLEASE READ](#please-read)
    - [Useful Resources](#useful-resources)

<!-- /TOC -->

## Introduction
### Abbreviations used in this document
- KPS: kube-prometheus-stack

### What is KPS?
KPS is a helm chart to deploy Grafana, [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator), and some additional services (i.e. prometheus-node-exporter, kube-state-metrics).  
  
### Problems with plain prometheus
The major drawback of a plain prometheus deployment is because all of the configurations are defined inside two configmaps (prometheus and alertmanager). Although it is possible to split some of the configurations into multiple files, not all of them provide such functionalities. This introduces several issues:
  - Prometheus and alertmanager instances need to be restarted on every config change
  - Invalid configurations will cause instances to crashloop
  - It is not possible for tenants in different namespaces to deploy their own alerts  

In addition, prometheus scrapes metrics whenever resources specify label such as `prometheus.io/scrape`. It means that in a single cluster, prometheus will have duplicated metrics if two or more instances are deployed.

Hence, we decided to use Prometheus operator to solve the above issues and come with the following features (quoting from the official docs):
> - Kubernetes Custom Resources: Use Kubernetes custom resources to deploy and manage Prometheus, Alertmanager, and related components.
> - Simplified Deployment Configuration: Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource.
> - Prometheus Target Configuration: Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.
> - Dynamic Admission Control: To prevent invalid Prometheus alerting and recording rules from causing failures in a deployed Prometheus instance, an admission webhook is provided to validate PrometheusRule resources upon initial creation or update.

## FAQ
### I want to setup an alert to my channel
You will need to configure a slack hook and two resources to get the alerts to work.
### How can I setup slack channel hook?
1. Decide on the slack channel for example #wcm-org-agora-alerts
2. From [Incoming WebHooks](https://woven-by-toyota.slack.com/apps/A0F7XDUAZ-incoming-webhooks) integration page, click “Add to Slack” button the left, then follow the wizard (mainly selecting the target channel)
3. Take the webhook URL generated by this, and add it as a k8s secret to the namespace where your AlertManager configurations will be
   1. Set cluster context as necessary, e.g. `kubectl config use-context lab`
   2. `read -s WEBHOOK_URL` & paste webhook url (to prevent it from going into shell command history)
   3. `kubectl create secret generic -n {your-namespace} slack-webhook --from-literal=address="$WEBHOOK_URL"`
4. Now when you configure the [alerting.yaml](alerting.yaml) resource the apiURL will point to the slack channel hook.

Note if you are configuring alerts just for one channel you don't need to specifiy the [channel variable](alerting.yaml#L19) in the alert.yaml example. This way if the channel name cnages in the future your alerts will still work.
 
### How do I setup k8s resources?
There are two k8s resources that needs to be defined, **AlertmanagerConfig** and **PrometheusRule**.  
- AlertmanagerConfig defines template, receiver, and alert interval
- PrometheusRule contains alerts defined in promQL query and description

General example with a description is defined in [alerting.yaml](alerting.yaml)  
In addition to that, we have some examples of how to monitor pods in [check-additional-workloads](check-additional-workloads)  
Recommended reference: [Awesome Prometheus alerts](https://awesome-prometheus-alerts.grep.to/rules.html#kubernetes)
  
more verbose example in [alerting-dev.yaml](alerting-dev.yaml)
  
### I want to monitor ...
- [pods](podmonitor.yaml)
- [services](servicemonitor.yaml)
  
### How we generate KPS resources ?
Similar to other observability services, we use an [import script](../../../k8s/common/observability-dev/bin/import) that uses helm template to generate resources. 

### Why are my alerts firing at incorrect times?
Four settings affect when your alerts fire.

The first one explains when the alert is created from a metric.

* [for: ](alerting.yaml#L55) - indicates how long your `expr` must be true before a metric becomes an alert.

The next three explain when and how often the created alerts should be fired.

* [groupWait: ](alerting.yaml#L35) - indicates how long to wait to buffer alerts of the same group before sending a notification initially.
* [groupInterval: ](alerting.yaml#L37) - indicates how long to wait before sending an alert that has been added to a group for which there has already been a notification.
* [repeatInterval: ](alerting.yaml#L39) -  how long to wait before re-sending a given alert that has already been sent in a notification.

In general, it's a good idea to make sure that `repeatInterval` is divisible by `groupInterval` otherwise, you might encounter a situation where the alert is not fired because it's not time to group things yet. Another thing to consider is that even if you configure your Alert times perfectly, you might still encounter situations where Alerts are not fired because of internal race conditions.

Check [this issue](https://github.com/prometheus/alertmanager/issues/2647#issuecomment-878749988) for a better explanation.


### Note for infra team
#### [Example: deploying alertmanager instance](./alertmanager)
#### [Example: deploying prometheus instance](./prometheus)
#### Upgrading prometheus-operator
- Notes:
  - When upgrading operator, flux may throw an error similar to this
  ```
  Job/observability-system/kube-prometheus-stack-admission-create dry-run failed, reason: Invalid, error: Job.batch "kube-prometheus-stack-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"kube-prometheus-stack-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"kube-prometheus-stack-admission-create", "app.kubernetes.io/instance":"kube-prometheus-stack", "app.kubernetes.io/managed-by":"Helm", "app.kubernetes.io/part-of":"kube-prometheus-stack", "app.kubernetes.io/version":"43.0.0", "chart":"kube-prometheus-stack-43.0.0", "controller-uid":"90873678-938d-4b71-bed1-410af3a6060a", "heritage":"Helm", "job-name":"kube-prometheus-stack-admission-create", "release":"kube-prometheus-stack"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.3.0", Command:[]string(nil), Args:[]string{"create", "--host=kube-prometheus-stack-operator,kube-prometheus-stack-operator.observability-system.svc", "--namespace=observability-system", "--secret-name=kube-prometheus-stack-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar(nil), Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(0xc03ae197a0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc05a27f898), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"kube-prometheus-stack-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc05277af00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
  ```
  - To resolve this error, simply delete `kube-prometheus-stack-admission-create` and `kube-prometheus-stack-admission-patch` Jobs.
  - These 2 jobs are responsible for creating certs for webhooks. This is safe to do, because flux will reconcile and recreate those Jobs.
  - Please let flux reconcile if you also notice prometheus instances are complaining about invalid certificates.
- References:
  - Jira: https://jira.tri-ad.tech/browse/CITYPF-3054
  - PR: https://github.tri-ad.tech/cityos-platform/cityos/pull/4241
  - https://medium.com/freethreads/fix-kubernetes-cron-job-invalid-spec-template-field-is-immutable-74f638ffb0aa

#### PLEASE READ
KPS helm chart comes with kube-state-metrics and prometheus-node-exporter helm charts.
The default values of the latter two charts may cause duplicate metrics when non-kps prometheus instance exists in the cluster.
Please disable this behavior in the helm values as below:

```yaml
kube-state-metrics:
  ...
  prometheusScrape: false
...
prometheus-node-exporter:
  ...
  service:
    ...
    annotations:
      prometheus.io/scrape: "false"
```

### Useful Resources
- [Types of CRD](https://github.com/prometheus-operator/prometheus-operator#customresourcedefinitions)
- [Delays on alerting](https://pracucci.com/prometheus-understanding-the-delays-on-alerting.html)
